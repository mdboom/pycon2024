<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>Measuring the performance of CPython</title>

    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/solarized.css" />

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/zenburn.css" />
  </head>
  <body>
    <div class="reveal">
      <header
        style="
          position: absolute;
          top: 20px;
          left: 20px;
          z-index: 500;
          font-size: 0.7em;
          font-family: Lato;
          text-transform: uppercase;
          font-weight: bold;
        "
      ></header>
      <div class="slides">
        <section>
          <h2>Measuring the<br />Performance of<br />CPython</h2>
          <p>Michael Droettboom</p>
          <p>
            <small>Microsoft<br />PyCon, May 18, 2024</small>
          </p>
        </section>
        <section>
          <h2>
            How do I make <i>my</i>
            <div data-id="py">Python code</div>
            <div data-id="faster">faster?</div>
          </h2>
        </section>
        <section>
          <img src="plots/runtime.svg" />
        </section>
        <section>
          <pre><code class="python" data-line-numbers="4-13|1-3">import numba

@numba.jit(nopython=True)
def mandel(x, y, max_iters):
	i = 0
	c = complex(x,y)
	z = 0.0j
	for i in range(max_iters):
		z = z * z + c
		if (z.real * z.real + z.imag * z.imag) >= 4:
			return i

	return 255
		  </code></pre>
        </section>
        <section>
          <h2>This is not that talk.</h2>
        </section>
        <section data-auto-animate>
          <h2>
            How do I make <i>all</i>
            <div data-id="py">Python code</div>
            <div data-id="faster">faster?</div>
          </h2>
        </section>
        <section>
          <img class="r-stretch" src="plots/compilation.svg" />
        </section>
        <section>
          <h2>This is not that talk.</h2>
        </section>
        <section data-auto-animate>
          <h2>
            How do I <i>know</i> I am making <i>all</i>
            <div data-id="py">Python code</div>
            <div data-id="faster">faster?</div>
          </h2>
        </section>
        <section>
          <h2 class="r-fit-text">This is <i>that</i> talk.</h2>
        </section>
        <section>
          <table>
            <tr>
              <td><img src="plots/stopwatch.png" /></td>
              <td style="vertical-align: middle">
                <ul>
                  <li class="fragment">What should we measure?</li>
                  <li class="fragment">How do we build a better stopwatch?</li>
                  <li class="fragment">How do we “scale up” the stopwatch?</li>
                </ul>
              </td>
            </tr>
          </table>
        </section>
        <section data-markdown="Teams.md"></section>
        <section>
          <h1>Part I:</h1>
          <h2>Measurement targets</h2>
        </section>
        <section>
          <h2>
            <i>Why don't we just...</i><br />measure the time spent in all
            Python programs?
          </h2>
        </section>
        <section data-background-image="plots/marathon.webp"></section>
        <section>
          <img src="plots/PAGE_LOAD_MS.png" />
          <img src="plots/firefox.svg" />
        </section>
        <section>
          <table>
            <tr>
              <td>
                <img src="plots/go.png" />
              </td>
              <td style="vertical-align: middle" class="fragment">
                “I am not suggesting that instrumentation be added by the Go
                compiler to all Go programs in the world: that’s clearly
                inappropriate.” <br />
                <div style="text-align: right">— Russ Cox</div>
              </td>
            </tr>
          </table>
        </section>
        <section>
          <h2>Too many uncontrolled variables</h2>
        </section>
        <section>
          <img src="plots/ChicagoWeather.png" />
        </section>
        <section>
          <h1 class="r-fit-text">Benchmarking</h1>
        </section>
        <section data-background-image="plots/robot-runner.jpeg"></section>
        <section>
          <h3>pyperformance suite</h3>
          <small>
            <code>
              2to3 aiohttp async_generators async_tree_cpu_io_mixed
              async_tree_cpu_io_mixed_tg async_tree_io async_tree_io_tg
              async_tree_memoization async_tree_memoization_tg async_tree_none
              async_tree_none_tg asyncio_tcp asyncio_tcp_ssl asyncio_websockets
              bench_mp_pool bench_thread_pool chameleon chaos comprehensions
              coroutines coverage create_gc_cycles crypto_pyaes dask deepcopy
              deepcopy_memo deepcopy_reduce deltablue django_template djangocms
              docutils dulwich_log fannkuch flaskblogging float gc_traversal
              generators genshi_text genshi_xml go gunicorn hexiom html5lib json
              json_dumps json_loads logging_format logging_silent logging_simple
              mako mdp meteor_contest mypy2 nbody nqueens pathlib pickle
              pickle_dict pickle_list pickle_pure_python pidigits pprint_pformat
              pprint_safe_repr pycparser pyflate pylint python_startup
              python_startup_no_site raytrace regex_compile regex_dna
              regex_effbot regex_v8 richards richards_super scimark_fft
              scimark_lu scimark_monte_carlo scimark_sor scimark_sparse_mat_mult
              spectral_norm sqlalchemy_declarative sqlalchemy_imperative
              sqlglot_normalize sqlglot_optimize sqlglot_parse sqlglot_transpile
              sqlite_synth sympy_expand sympy_integrate sympy_str sympy_sum
              telco thrift tomli_loads tornado_http typing_runtime_protocols
              unpack_sequence unpickle unpickle_list unpickle_pure_python
              xml_etree_generate xml_etree_iterparse xml_etree_parse
              xml_etree_process
            </code>
          </small>
        </section>
        <section>
          <img class="r-stretch" src="plots/family-tree.svg" />
        </section>
        <section>
          <h2>Benchmark ontology</h2>
          <table>
            <tr>
              <td><img src="plots/handp.jpeg" /></td>
              <td style="vertical-align: middle">
                <ul>
                  <li class="fragment">Toy benchmarks</li>
                  <li class="fragment">Real applications</li>
                  <ul>
                    <li class="fragment">Modified applications</li>
                    <li class="fragment">Application kernels</li>
                  </ul>
                  <li class="fragment">+ Microbenchmarks</li>
                </ul>
              </td>
            </tr>
          </table>
        </section>
        <section data-state="toy">
          <h3>nqueens</h3>
          <h3><img width="300" height="300" src="plots/nqueens.png" /></h3>
        </section>
        <section data-state="toy">
          <h3>richards</h3>
          <pre><code class="python">
"""
based on a Java version:
 Based on original version written in BCPL by Dr Martin Richards
 in 1981 at Cambridge University Computer Laboratory, England
 and a C++ version derived from a Smalltalk version written by
 L Peter Deutsch.
 Java version:  Copyright (C) 1995 Sun Microsystems, Inc.
 Translation from C++, Mario Wolczko
 Outer loop added by Alex Jacoby
"""
			</code></pre>
        </section>
        <section data-state="toy">
          <h3>richards</h3>
          <pre><code class="text">
A Benchmark Test for Systems Implementation Languages
-----------------------------------------------------

This distribution is currently being put together and will be in a
more polished state in due course.

It was designed in 1980 to test the efficiency of implementations of
programming languages.

Please feel free to re-implement this benchmark in any language you
choose.  I would be happy to incorporate such translations into this
distribution together with their timing/size results.

Martin Richards
23 February 2007
			</code></pre>
        </section>
        <section data-state="toy">
          <div class="fragment">
            <p style="text-align: justify">
              “In my opinion, benchmark suites should only contain [application
              benchmarks].”
            </p>
            <p style="text-align: right">
              — Nicolas Nethercote
              <small>(A browser benchmarking manifesto)</small>
            </p>
          </div>

          <div class="fragment">
            <p style="text-align: justify">
              “Striving for even higher benchmark scores on Octane translated
              into increasingly-marginal improvements in the performance of real
              web pages.”
            </p>
            <p style="text-align: right">
              — V8 Team <small>(Retiring Octane)</small>
            </p>
          </div>
        </section>
        <section data-state="toy">
          <p style="text-align: justify">
            “You don't have time to inspect the source code of real applications
            to check that different implementations are kind-of comparable.
          </p>

          <p style="text-align: justify">
            You do have time to inspect 100-line programs. You do have time to
            write 100-line programs. You still might have something to learn
            from how other people write 100-line programs.”
          </p>
          <p style="text-align: right">— Issac Gouy (Debian Benchmarks Game)</p>
        </section>
        <section data-state="micro">
          <h3>comprehensions</h3>
          <pre><code class="python">
widgets = [w for w in widgets if not self._is_big_spinny(w)]
id_to_widget = {w.widget_id: w for w in widgets}
id_to_derived = {
	w.widget_id: [id_to_widget.get(dwid) for dwid in w.derived_widget_ids]
	for w in widgets
}
		  </code></pre>
        </section>
        <section data-state="micro">
          <h3>gc (garbage collector)</h3>
          <ul>
            <li>Deliberately creates reference cycles between many objects</li>
            <li>Measures how long it takes to collect them</li>
          </ul>
        </section>
        <section data-state="apps">
          <h3>djangocms</h3>
          <table>
            <tr>
              <td>
                <img src="plots/djangocms.png" />
              </td>
              <td style="vertical-align: middle">
                For an example site, how long does it take to serve a webpage?
              </td>
            </tr>
          </table>
        </section>
        <section data-state="modified">
          <h3>pylint</h3>
          <ol>
            <li>Load a large Python file into memory</li>
            <li>
              Lint it
              <img
                src="plots/stopwatch.png"
                width="64"
                height="64"
                class="fragment"
              />
            </li>
          </ol>
        </section>
        <section data-state="kernels">
          <h3>docutils</h3>
          Convert a large reStructuredText file to HTML.
          <br />
          (An important subcomponent of generating Sphinx documentation).
        </section>
        <section data-state="kernels">
          <h3>sqlalchemy</h3>
          <img src="plots/sqlalchemy.svg" />
        </section>
        <section>
          <table>
            <tr>
              <td>
                <img src="plots/greenlet-python3.13.png" />
              </td>
              <td>PR: Port to Python 3.13</td>
            </tr>
          </table>
        </section>
        <section>
          <h3><i>All</i> benchmarks are useful, just in different ways.</h3>
          <table style="border: 1px solid black">
            <tr>
              <td><b>Application</b></td>
              <td><b>Toy / Micro</b></td>
            </tr>
            <tr>
              <td>By <em>how much</em> did things get faster?</td>
              <td><em>Why</em> did things get faster?</td>
            </tr>
            <tr>
              <td>Harder to inspect and maintain.</td>
              <td>Easier to inspect and maintain.</td>
            </tr>
          </table>
        </section>
        <section>
          <img src="plots/by_categories.svg" class="r-stretch" />
        </section>
        <section>
          Where do benchmarks spend their time?
          <img src="plots/tier1.pie.svg" class="r-stretch" />
        </section>
        <section>
          <img src="plots/tier1.svg" class="r-stretch" />
        </section>
        <section>
          <img src="plots/by_perf_categories.svg" class="r-stretch" />
        </section>
        <section>
          <img src="plots/compare.svg" width="100%" />
        </section>
        <section>
          <h2>Call to action:</h2>
          <h3>
            If you care about a particular workload going faster, please donate
            some benchmarks
          </h3>
          <h4>(Make a PR to pyperformance)</h4>
        </section>
        <section>
          <h1>Part II:</h1>
          <h2>Building a better stopwatch</h2>
        </section>
        <section>
          <img src="plots/benchmarking.svg" />
        </section>
        <section>
          <h2>A/A testing</h2>
          <ul>
            <li class="fragment">Average noise: ± 1.0%</li>
            <li class="fragment">Most performance improvements are < 1.0%</li>
          </ul>
        </section>
        <section data-state="noise">
          <h3>“Other things” happening on the system</h3>
          <ul>
            <li class="fragment">Multi-tasking by the OS</li>
            <ul>
              <li class="fragment">CPU core isolation</li>
              <li class="fragment">CPU core pinning</li>
            </ul>
            <li class="fragment">Virtual machines</li>
            <ul>
              <li class="fragment">
                Typical cloud-computing environments introduce too much noise
              </li>
            </ul>
          </ul>
        </section>
        <section data-state="noise">
          <h3>Thermal management</h3>
          <img src="plots/cpufreq.png" class="r-stretch" />
        </section>
        <section data-state="noise">
          <h3>Memory layout</h3>
          <ul>
            <li class="fragment">
              Each <em>run</em> of CPython has a unique memory layout
            </li>
            <ul>
              <li class="fragment">
                Address space layout randomization (ASLR)
              </li>
            </ul>
            <li class="fragment">
              Each <em>build</em> of CPython has a unique structure
            </li>
            <ul>
              <li class="fragment">Profile-guided optimization (PGO)</li>
            </ul>
          </ul>
        </section>
        <section data-state="noise">
          <h3>The benchmarks themselves</h3>
          <img src="plots/compare.svg" />
        </section>
        <section>
          <h2>pyperf: The better stopwatch</h2>
          <ul>
            <li>
              Encapsulates all of the ways to run more consistent benchmarks
            </li>
          </ul>
        </section>
        <section>
          <h2>Using statistics to account for noise</h2>
          <ul>
            <li>Hierarchical performance testing (HPT)</li>
            <ul>
              <li>Used in SPEC benchmarking suite.</li>
              <li>
                Only include results in the overall average when they are X%
                more likely to be different than the the null hypothesis.
              </li>
              <li>
                “The system measured by the benchmarks is 90% likely to be 1%
                faster.”
              </li>
            </ul>
          </ul>
        </section>
        <section>
          <h1>Part III:</h1>
          <h2>Sharing the stopwatch</h2>
        </section>
        <section>
          <h3>Continuous benchmarking</h3>
          <ul>
            <li class="fragment">Everyone uses the same bare-metal machines</li>
            <li class="fragment">
              Built with Github Actions and self-hosted runners
            </li>
          </ul>
        </section>
        <section>
          <img src="plots/action-runner.png" class="r-stretch" />
        </section>
        <section>
          <h3>CB vs. CI</h3>
          <table style="border: 1px solid black">
            <tr>
              <td></td>
              <td><b>Continuous integration</b></td>
              <td><b>Continuous benchmarking</b></td>
            </tr>
            <tr>
              <td><b>Setup</b></td>
              <td>Mature</td>
              <td>DIY</td>
            </tr>
            <tr>
              <td><b>Cost</b></td>
              <td>Free 2000 minutes for OSS, $0.008/min</td>
              <td>Own machines or “call your sales rep”</td>
            </tr>
            <tr>
              <td><b>Security</b></td>
              <td>Safe to run arbitrary PRs</td>
              <td>Must only run trusted PRs</td>
            </tr>
          </table>
        </section>
        <section>
          <h2>Data-driven<br />decision-making</h2>
          <img src="plots/workflow.svg" />
        </section>
        <section>
          <h2>Data-driven<br />idea generation</h2>
          <img src="plots/workflow2.svg" />
        </section>
        <section>
          <section data-background-image="plots/runners.jpeg">
            <h1 style="color: white; -webkit-text-stroke: 1px black">
              Thanks!
            </h1>
            <h3 style="color: white; -webkit-text-stroke: 1px black">
              (Submit your benchmarks!)
            </h3>
          </section>
        </section>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],

        markdown: {
          smartypants: true,
        },

        transition: "none",
      });
    </script>
  </body>
</html>
